# QK-Clip技术概览

## 背景问题
在训练超大规模语言模型（如万亿参数的Kimi K2）时，遇到了**MaxLogit爆炸**现象：
- 注意力机制中$\boldsymbol{Q}\boldsymbol{K}^{\top}$的最大绝对值持续增长
- 可能导致梯度尖峰甚至训练崩溃
- 在使用Muon优化器训练大模型时尤为严重

## 现有方案局限性
1. **Weight Decay** - 在大模型中控制力不足
2. **Softcap** - 只是问题转移，未根本解决  
3. **QK-Norm** - 不兼容MLA架构（Multi-head Latent Attention）

## QK-Clip核心方案
**直接监控策略**：以MaxLogit值作为触发信号，超过阈值时直接对权重进行缩放

### 技术细节
- **阈值设定**：τ = 100
- **Per-Head监控**：避免无关头部受影响
- **MLA优化**：
  - qc/kc部分：缩放因子 $\sqrt{\tau/S_{\max}}$
  - qr部分：缩放因子 $\tau/S_{\max}$

### 实际效果
在Kimi K2训练过程中：
- 7k步后开始触发
- 与Muon优化器形成70k步"拉锯战"
- 后期MaxLogit自动稳定，无需继续干预
- **对模型效果无损**

## 理论解释
Muon优化器更易引起MaxLogit爆炸的原因：
- Muon更新量为满秩，与参数权重"碰撞概率"更高
- 注意力机制的双线性形式放大了不稳定性

## 通用价值
QK-Clip体现了"**哪里不稳定就裁剪哪里**"的设计思路，可推广到其他训练不稳定问题的解决方案中。

---
*本概览基于@qk_clip_blog.md中的技术博客整理*