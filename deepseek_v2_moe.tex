# copy from: https://arxiv.org/abs/2405.04434

\subsection{\dsmoe{}: Training Strong Models at Economical Costs}

\subsubsection{Basic Architecture}

For FFNs, we employ the \dsmoe{} architecture~\citep{deepseekmoe}. 
\dsmoe{} has two key ideas: segmenting experts into finer granularity for higher expert specialization and more accurate knowledge acquisition, and isolating some shared experts for mitigating knowledge redundancy among routed experts. 
With the same number of activated and total expert parameters, \dsmoe{} can outperform conventional MoE architectures like GShard~\citep{gshard} by a large margin. 

Let $\mathbf{u}_{t}$ be the FFN input of the $t$-th token, we compute the FFN output $\mathbf{h}_{t}^{\prime}$ as follows:
\begin{align}
    \mathbf{h}_{t}^{\prime} & = \mathbf{u}_{t} + \sum_{i=1}^{N_{s}} {\operatorname{FFN}^{(s)}_{i}\left( \mathbf{u}_{t} \right)} + \sum_{i=1}^{N_r} {g_{i,t} \operatorname{FFN}^{(r)}_{i}\left( \mathbf{u}_{t} \right)}, \\
    g_{i,t} & = \begin{cases} 
    s_{i,t}, & s_{i,t} \in \operatorname{Topk} (\{ s_{j, t} | 1 \leq j \leq N_r \}, K_{r}), \\
    0, & \text{otherwise}, 
    \end{cases} \\
    s_{i,t} & = \operatorname{Softmax}_i \left( {\mathbf{u}_{t}}^{T} \mathbf{e}_{i} \right),
\end{align}
where $N_{s}$ and $N_r$ denote the numbers of shared experts and routed experts, respectively; 
$\operatorname{FFN}^{(s)}_{i}(\cdot)$ and $\operatorname{FFN}^{(r)}_{i}(\cdot)$ denote the $i$-th shared expert and the $i$-th routed expert, respectively; 
$K_{r}$ denotes the number of activated routed experts; 
$g_{i,t}$ is the gate value for the $i$-th expert; 
$s_{i,t}$ is the token-to-expert affinity; 
$\mathbf{e}_{i}$ is the centroid of the $i$-th routed expert in this layer; 
and $\operatorname{Topk}(\cdot, K)$ denotes the set comprising $K$ highest scores among the affinity scores calculated for the $t$-th token and all routed experts.

\subsubsection{Device-Limited Routing}

We design a device-limited routing mechanism to bound MoE-related communication costs. 
When expert parallelism is employed, the routed experts will be distributed across multiple devices. 
For each token, its MoE-related communication frequency is proportional to the number of devices covered by its target experts.
Due to the fine-grained expert segmentation in \dsmoe{}, the number of activated experts can be large, so the MoE-related communication will be more costly if we apply expert parallelism. 

For \dsvii{}, beyond the naive top-K selection of routed experts, we additionally ensure that the target experts of each token will be distributed on at most $M$ devices. 
To be specific, for each token, we first select $M$ devices that have experts with the highest affinity scores in them. 
Then, we perform top-K selection among experts on these $M$ devices. 
In practice, we find that when $M \geq 3$, the device-limited routing can achieve a good performance roughly aligned with the unrestricted top-K routing. 

\subsubsection{Auxiliary Loss for Load Balance}

We take the load balance into consideration for automatically learned routing strategies. 
Firstly, unbalanced load will raise the risk of routing collapse~\citep{moe}, preventing some experts being fully trained and utilized. 
Secondly, when expert parallelism is employed, unbalanced load will diminish computation efficiency. 
During the training of \dsvii{}, we design three kinds of auxiliary losses, for controlling expert-level load balance ($\mathcal{L}_{\mathrm{ExpBal}}$), device-level load balance ($\mathcal{L}_{\mathrm{DevBal}}$), and communication balance ($\mathcal{L}_{\mathrm{CommBal}}$), respectively. 

\paragraph{Expert-Level Balance Loss.}
We use an expert-level balance loss~\citep{switch,gshard} to mitigate the risk of routing collapse:
\begin{align}
    \mathcal{L}_{\mathrm{ExpBal}} & = \alpha_1 \sum_{i=1}^{N_r}{f_i P_i}, \\
    f_i & = \frac{N_r}{K_r T} \sum_{t=1}^{T}{ \mathds{1}( \text{Token $t$ selects Expert $i$} )}, \\
    P_i & = \frac{1}{T} \sum_{t=1}^{T}{s_{i,t}},
\end{align}
where $\alpha_1$ is a hyper-parameter called expert-level balance factor; 
$\mathds{1}(\cdot)$ denotes the indicator function; 
and $T$ denotes the number of tokens in a sequence. 

\paragraph{Device-Level Balance Loss.}
In addition to the expert-level balance loss, we additionally design a device-level balance loss to ensure balanced computation across different devices.
In the training process of \dsvii{}, we partition all routed experts into $D$ groups $\{\mathcal{E}_1, \mathcal{E}_2, ..., \mathcal{E}_D \}$, and deploy each group on a single device. 
The device-level balance loss is computed as follows:
\begin{align}
    \mathcal{L}_{\mathrm{DevBal}} & = \alpha_{2} \sum_{i=1}^{D}{f_i^{\prime} P_i^{\prime}}, \\
    f_i^{\prime} & = \frac{1}{|\mathcal{E}_i|} \sum_{j \in \mathcal{E}_i}{ f_j }, \\
    P_i^{\prime} & = \sum_{j \in \mathcal{E}_i}{ P_j },
\end{align}
where $\alpha_{2}$ is a hyper-parameter called device-level balance factor. 

\paragraph{Communication Balance Loss.}
Finally, we introduce a communication balance loss to ensure that the communication of each device is balanced. 
Although the device-limited routing mechanism guarantees that the sending communication of each device is bounded, if a certain device receives more tokens than other devices, the practical communication efficiency will also be affected. 
In order to mitigate this issue, we design a communication balance loss as follows: 
\begin{align}
    \mathcal{L}_{\mathrm{CommBal}} & = \alpha_{3} \sum_{i=1}^{D}{f_i^{\prime\prime} P_i^{\prime\prime}}, \\
    f_i^{\prime\prime} & = \frac{D}{M T} \sum_{t=1}^{T}{ \mathds{1}( \text{Token $t$ is sent to Device $i$} )}, \\
    P_i^{\prime\prime} & = \sum_{j \in \mathcal{E}_i}{ P_j },
\end{align}
where $\alpha_{3}$ is a hyper-parameter called communication balance factor. 
The device-limited routing mechanism operates on the principle of ensuring that each device transmits at most $MT$ hidden states to other devices. 
Simultaneously, the communication balance loss is employed to encourage each device to receive around $MT$ hidden states from other devices. 
The communication balance loss guarantees a balanced exchange of information among devices, promoting efficient communications. 

\subsubsection{Token-Dropping Strategy}

While balance losses aim to encourage a balanced load, it is important to acknowledge that they cannot guarantee a strict load balance. 
In order to further mitigate the computation wastage caused by unbalanced load, we introduce a device-level token-dropping strategy during training. 
This approach first computes the average computational budget for each device, which means that the capacity factor for each device is equivalent to 1.0. 
Then, inspired by \citet{bpr}, we drop tokens with the lowest affinity scores on each device until reaching the computational budget. 
In addition, we ensure that the tokens belonging to approximately 10\% of the training sequences will never be dropped. 
In this way, we can flexibly decide whether to drop tokens during inference according to the efficiency requirements, and always ensure consistency between training and inference. 
